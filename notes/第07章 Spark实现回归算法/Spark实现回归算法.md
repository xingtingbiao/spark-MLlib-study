Spark实现回归算法

目录: 
1. 回归分析概述                 2. 线性回归算法概述

3. 线性回归算法的原理           4. 最小二乘法

5. 随机梯度下降                 6. 实战Spark预测房价

7. 逻辑回归算法概述             8. 逻辑回归算法原理

9. 正则化原理                   10. 实战Spark逻辑回归

11. 保序回归算法概述            12. 保序回归算法原理

13. 实战一个保序回归数据分析






一. 回归分析概述 
  1) 回归分析介绍
    a. 回归与分类类似, 只不过回归的预测结果是<连续>的, 而分类的预测结果是<离散>的
    b. 这样, 也就使得很多回归与分类的模型可以经过改动而通用
    c. 正因如此, 对于回归和分类中基本原理相同的或类似的模型, 我们不再具体讲解
    

  2) Spark实现的回归算法
    a. Spark实现的回归算法很丰富, 有很多模型同样可以用于分类, 我们将在后面的分类中具体介绍. 
    其官方文档介绍如图所示: http://spark.apache.org/docs/2.3.3/mllib-classification-regression.html  --> Classification and regression






二. 线性回归算法概述
  1) 线性回归简介
    a. 在回归分析中, '自变量Y'与'因变量X'之间满足或基本满足<线性关系>, 可以使用线性模型进行拟合.
    b. 如回归分析中, 只有一个'自变量'的即为<一元线性回归>, 其自变量与因变量之间关系可以用一条直线近似表示.
    c. 同理, 对于多变量的回归称为<多元线性回归>, 其可以用一个平面或者超平面来表示.
  
  2) 使用线性回归的前提条件
    a. 自变量与因变量之间具有线性趋势, 我们在前面介绍过相关系数.
    b. 独立性: 因变量之间的取值相互独立, 不存在关联

  3) 线性回归的例子
    a. 例如探究沸点与气压的关系, 研究浮力与表面积之间的关系, 物理上经典的探索力与加速度之间的关系.






三. 线性回归算法的原理 
  1) 回顾机器学习模型
    a. 对于统计学习来讲, 机器学习模型就是一个函数表达式, 其训练过程就是在不断更新这个函数式的'参数', 
       以便这个函数能够对未知数据产生最好的预测结果.

    b. 机器学习的这个过程, 与人的学习过程原理是一样的, 都是先学习而后使用, 故归属于人工智能领域.

  2) 何为好的预测效果?
    a. 前面说 "以便达到最好的预测效果", 那么如何量化 "好的预测效果" 呢？
    b. 衡量预测效果好坏的函数称为<代价函数>(cost function), 或损失函数(loss function)
    例如: 用一个模型预测是否会下雨, 如果模型预测错误一天, 则损失函数加1, 那么机器学习算法的直接目标就是想方设法调节这个函数的参数,
          以便能够使预测错误的天数减少, 也就是降低损失函数值, 同时, 提供了预测的'准确率'

  3) 再谈线性回归
    a. 线性回归是最简单的数学模型之一.

    b. 线性回归的步骤是先用既有的数据, 探索自变量X与因变量Y之间存在的关系, 这个关系就是线性回归模型中的参数. 有了它, 
       我们就可以用这个模型对未知数据进行预测了.

    c. 机器学习的模型基本的训练过程亦是如此, 属于<监督学习>

  4) 线性回归模型
    a. 线性回归的数学表达式: 
                  y = w*x + b
                  y = wT * X
    b. 上式分别为一元线性回归与写成矩阵形式的线性回归模型







四. 最小二乘法
  1) 何为最小二乘法
    a. 最小二乘法又称平方差, 通过最小化<残差平方和>来找到最佳的函数匹配.
       残差: 预测值和观察值之间的差, 类似于误差: 预测结果和实际结果的差

    b. 也就是说, 最小二乘法以残差的平方和作为<损失函数>, 用于衡量模型的好坏.

    c. 利用最小二乘法可以实现对'曲线'的拟合.

  2) 最小二乘法的原理
    a. 以一元线性回归为例, 演示推导过程:
       详见图: 最小二乘法推导过程






五. 随机梯度下降 
  1) 何为随机梯度下降
    a. 随机梯度下降(SGD)是一种机器学习中常用的优化方法.
    b. 它是通过'不断迭代更新'的手段, 来寻找某一个函数的'全局最优解'的方法.
    c. 与最小二乘法类似, 都是优化算法, 随机梯度下降特别适合变量众多, 受控系统复杂的模型, 
       尤其在深度学习中具有十分重要的作用.

  2) 从梯度说起
    a. 梯度是微积分中的一个算子, 用来求某函数在该点处沿着哪条路径'变化最快', 通俗理解就是在哪个路径上的几何形态更为'陡峭'.

    b. 其数学表达式为(以二元函数为例): 
       详见图: 梯度的数学表达式.png

  3) 随机梯度下降原理
    a. 线性模型的梯度下降推导过程: 
       详见图: 随机梯度下降原理.png
       参考: https://blog.csdn.net/weixin_39445556/article/details/83661219

  4) 随机梯度下降优点:
    a. 随机梯度下降的 "随机" 体现在进行梯度计算的样本是随机抽取的N个, 与直接采用全部样本相比, 这样'计算量更小'.
    b. 随机梯度下降善于解决大量训练样本的情况.
    c. '学习率'决定了梯度下降的速度, 同时, 在SGD的基础上引入了 "动量" 的概念, 从而进一步加速收敛速度的优化算法也陆续本提出.





